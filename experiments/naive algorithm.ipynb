{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5 (default, Jan 27 2021, 15:41:15) \n",
      "[GCC 9.3.0] on linux\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "sys.path.extend(['/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection', '/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection'])\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "cwd = os.getcwd()\n",
    "test_dir = Path(cwd).parent/\"duplication\"/\"test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duplication.run import *\n",
    "from duplication.similarity_metrics import jaccard\n",
    "from duplication.detectors import NaiveDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DetectionResult:\n",
    "    \"\"\"\n",
    "    Class for storing results of detection\n",
    "    \"\"\"\n",
    "    clones: List[Tuple[EntityData, EntityData, float]]\n",
    "\n",
    "\n",
    "class EntityTypes(Enum):\n",
    "    Function = 0\n",
    "    Class = 1\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EntityData:\n",
    "    entity_type: EntityTypes\n",
    "    path: str\n",
    "    lang: str\n",
    "    bag_of_tokens: List[str]\n",
    "\n",
    "\n",
    "\n",
    "File = Tuple[str, str]\n",
    "\n",
    "\n",
    "class Detector:\n",
    "    @staticmethod\n",
    "    def extract_entity_from_object_data(obj: ObjectData, path, lang) -> EntityData:\n",
    "        entity_type = \"\"\n",
    "        if obj.object_type == ObjectTypes.FUNCTION:\n",
    "            entity_type = \"function\"\n",
    "        if obj.object_type == ObjectTypes.CLASS:\n",
    "            entity_type = \"class\"\n",
    "\n",
    "        entity_tokens = get_identifiers_sequence_from_code(obj.content, lang)\n",
    "        return EntityData(entity_type, path, lang, entity_tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_entity_from_file_data(file: FileData) -> EntityData:\n",
    "        entity_path = file_data.path\n",
    "        entity_lang = file_data.lang\n",
    "        return [Detector.extract_entity_from_object_data(obj, entity_path, entity_lang) for obj in file_data.objects]\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_entities_from_files_data(files: List[FileData]) -> List[EntityData]:\n",
    "        def acc(value: List[EntityData], file_data: FileData) -> List[EntityData]:\n",
    "            value.extend(Detector.extract_entity_from_file_data(file_data))\n",
    "            return value\n",
    "        return functools.reduce(acc, files, [])\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_data_from_files(files: List[File]) -> List[FileData]:\n",
    "        return [get_data_from_file(file, lang, True, False) for file, lang in files]\n",
    "        \n",
    "    def fill_fields(self, directory: str, granularity: str):\n",
    "        lang2files = recognize_languages_dir(directory)\n",
    "        files = transform_files_list(lang2files, granularity, None)\n",
    "        self.files = [(get_full_path(file, directory), lang) for file, lang in files]\n",
    "        self.files_data = self.extract_data_from_files(self.files)\n",
    "        self.entities = self.extract_entities_from_files_data(self.files_data)\n",
    "    \n",
    "    def detect(self, directory: str, granularity: str) -> List[DetectionResult]:\n",
    "        raise NotImplemented\n",
    "        \n",
    "        \n",
    "class NaiveDetector(Detector):\n",
    "    def detect(self, directory: str, threshold: float, granularity: str) -> DetectionResult:\n",
    "        self.fill_fields(directory, granularity)\n",
    "        clones = []\n",
    "        for i in range(len(self.entities)):\n",
    "            entity = self.entities[i]\n",
    "            for j in range(i + 1, len(self.entities)):\n",
    "                candidate = self.entities[j]\n",
    "                if entity == candidate or entity.lang != candidate.lang:\n",
    "                    continue\n",
    "                \n",
    "                sim = jaccard(entity.bag_of_tokens, candidate.bag_of_tokens)\n",
    "                if sim > threshold:\n",
    "                    clones.append((entity, candidate, sim))\n",
    "        return DetectionResult(clones)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_detector = NaiveDetector()\n",
    "clones = naive_detector.detect(test_dir.parent, 0.65, \"functions\").clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/test_data/test_file.java', lang='Java', bag_of_tokens=['HelloWorld', 'main', 'String', 'args', 'System', 'out', 'println', 'SomeFunction', 'a', 'System', 'out', 'println', 'a']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/test_data/test_file.java', lang='Java', bag_of_tokens=['main', 'String', 'args', 'System', 'out', 'println']),\n",
       "  0.6923076923076923),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/test_data/test_file.java', lang='Java', bag_of_tokens=['HelloWorld', 'main', 'String', 'args', 'System', 'out', 'println', 'SomeFunction', 'a', 'System', 'out', 'println', 'a']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/test_data/test_file.java', lang='Java', bag_of_tokens=['SomeFunction', 'a', 'System', 'out', 'println', 'a']),\n",
       "  0.6923076923076923),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.js', lang='JavaScript', bag_of_tokens=['Rectangle', 'height', 'width', 'height', 'height', 'width', 'width']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.js', lang='JavaScript', bag_of_tokens=['calcArea', 'height', 'width']),\n",
       "  0.8571428571428571),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/detectors.py', lang='Python', bag_of_tokens=['extract_entities_from_files_data', 'files', 'List', 'FileData', 'List', 'EntityData', 'acc', 'value', 'List', 'EntityData', 'file_data', 'FileData', 'List', 'EntityData', 'value', 'extend', 'Detector', 'extract_entity_from_file_data', 'file_data', 'value', 'functools', 'reduce', 'acc', 'files']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/detectors.py', lang='Python', bag_of_tokens=['acc', 'value', 'List', 'EntityData', 'file_data', 'FileData', 'List', 'EntityData', 'value', 'extend', 'Detector', 'extract_entity_from_file_data', 'file_data', 'value']),\n",
       "  0.7916666666666666),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/detectors.py', lang='Python', bag_of_tokens=['NaiveDetector', 'Detector', 'detect', 'self', 'directory', 'str', 'threshold', 'float', 'granularity', 'str', 'DetectionResult', 'self', 'fill_fields', 'directory', 'granularity', 'clones', 'i', 'range', 'len', 'self', 'entities', 'entity', 'self', 'entities', 'i', 'j', 'range', 'i', 'len', 'self', 'entities', 'candidate', 'self', 'entities', 'j', 'entity', 'candidate', 'entity', 'lang', 'candidate', 'lang', 'sim', 'jaccard', 'entity', 'bag_of_tokens', 'candidate', 'bag_of_tokens', 'sim', 'threshold', 'clones', 'append', 'entity', 'candidate', 'sim', 'DetectionResult', 'clones']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/detectors.py', lang='Python', bag_of_tokens=['detect', 'self', 'directory', 'str', 'threshold', 'float', 'granularity', 'str', 'DetectionResult', 'self', 'fill_fields', 'directory', 'granularity', 'clones', 'i', 'range', 'len', 'self', 'entities', 'entity', 'self', 'entities', 'i', 'j', 'range', 'i', 'len', 'self', 'entities', 'candidate', 'self', 'entities', 'j', 'entity', 'candidate', 'entity', 'lang', 'candidate', 'lang', 'sim', 'jaccard', 'entity', 'bag_of_tokens', 'candidate', 'bag_of_tokens', 'sim', 'threshold', 'clones', 'append', 'entity', 'candidate', 'sim', 'DetectionResult', 'clones']),\n",
       "  0.9642857142857143),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/test_data/test_file_clone1.py', lang='Python', bag_of_tokens=['some_func1', 'a', 's', 'x', 'a', 's', 'x', 's']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/test_data/test_file_clone2.py', lang='Python', bag_of_tokens=['some_func1', 'a', 's', 'x', 'a', 's', 'x', 's', 'x', 's']),\n",
       "  0.8),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_enry_dir']),\n",
       "  0.8333333333333334),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  0.9166666666666666),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_tree_sitter_dir']),\n",
       "  0.75),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'dirname', '__file__']),\n",
       "  0.8333333333333334),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'tree_sitter_dir', 'get_tree_sitter_dir', 'bin_loc', 'os', 'path', 'join', 'tree_sitter_dir', 'bin_loc']),\n",
       "  0.6666666666666666),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_enry_dir']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_tree_sitter_dir']),\n",
       "  0.7777777777777778),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/language_recognition/utils.py', lang='Python', bag_of_tokens=['get_enry', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_enry_dir']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'dirname', '__file__']),\n",
       "  0.6666666666666666),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_tree_sitter_dir']),\n",
       "  0.8333333333333334),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'dirname', '__file__']),\n",
       "  0.9166666666666666),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'os', 'path', 'dirname', '__file__']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'tree_sitter_dir', 'get_tree_sitter_dir', 'bin_loc', 'os', 'path', 'join', 'tree_sitter_dir', 'bin_loc']),\n",
       "  0.75),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_tree_sitter_dir']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.py', lang='Python', bag_of_tokens=['get_tree_sitter_dir', 'str', 'os', 'path', 'abspath', 'os', 'path', 'dirname', '__file__']),\n",
       "  0.7777777777777778),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/parsing/utils.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'os', 'path', 'abspath', 'os', 'path', 'join', 'get_tree_sitter_dir']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_files/test.py', lang='Python', bag_of_tokens=['get_tree_sitter_so', 'str', 'tree_sitter_dir', 'get_tree_sitter_dir', 'bin_loc', 'os', 'path', 'join', 'tree_sitter_dir', 'bin_loc']),\n",
       "  0.8),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/saver.py', lang='Python', bag_of_tokens=['OutputFormats', '__init__', 'self', 'output_format', 'str', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'output_format', 'self', 'save_wabbit', 'reps2files', 'mode', 'gran', 'output_dir', 'filename', 'output_format', 'self', 'save_json', 'reps2files', 'mode', 'gran', 'output_dir', 'filename', 'classmethod', 'save_wabbit', 'cls', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'counter_to_wabbit', 'tokens_counter', 'Counter', 'str', 'sorted_tokens', 'sorted', 'tokens_counter', 'items', 'key', 'itemgetter', 'reverse', 'formatted_tokens', 'token', 'sorted_tokens', 'formatted_tokens', 'append', 'format', 'token', 'token', 'count', 'str', 'token', 'join', 'formatted_tokens', 'sequence_to_wabbit', 'sequence', 'Union', 'List', 'str', 'List', 'IdentifierData', 'identifiers_type', 'IdentifiersTypes', 'str', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'join', 'sequence', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'formatted_tokens', 'token', 'sequence', 'parameters', 'join', 'str', 'parameter', 'parameter', 'token', 'start_byte', 'token', 'start_line', 'token', 'start_column', 'formatted_tokens', 'append', 'format', 'token', 'token', 'identifier', 'parameters', 'parameters', 'join', 'formatted_tokens', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'output_dir', 'filename', 'fout', 'gran', 'repository_name', 'reps2files', 'keys', 'repository_tokens', 'counter_to_wabbit', 'merge_bags', 'reps2files', 'repository_name', 'len', 'repository_tokens', 'fout', 'write', 'format', 'name', 'repository_name', 'tokens', 'repository_tokens', 'gran', 'repository_name', 'reps2files', 'values', 'file', 'repository_name', 'mode', 'file_tokens', 'counter_to_wabbit', 'Counter', 'file', 'identifiers', 'file_tokens', 'sequence_to_wabbit', 'file', 'identifiers', 'file', 'identifiers_type', 'len', 'file_tokens', 'fout', 'write', 'format', 'name', 'file', 'path', 'tokens', 'file_tokens', 'repository_name', 'reps2files', 'values', 'file', 'repository_name', 'obj', 'file', 'objects', 'gran', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'gran', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'mode', 'object_tokens', 'counter_to_wabbit', 'Counter', 'obj', 'identifiers', 'object_tokens', 'sequence_to_wabbit', 'obj', 'identifiers', 'obj', 'identifiers_type', 'len', 'object_tokens', 'fout', 'write', 'format', 'name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'tokens', 'object_tokens', 'classmethod', 'save_json', 'cls', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'res', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'output_dir', 'filename', 'fout', 'gran', 'repository_name', 'reps2files', 'keys', 'repository_tokens', 'merge_bags', 'reps2files', 'repository_name', 'len', 'repository_tokens', 'res', 'repository_name', 'repository_tokens', 'gran', 'repository_name', 'reps2files', 'keys', 'res', 'repository_name', 'file', 'reps2files', 'repository_name', 'len', 'file', 'identifiers', 'mode', 'res', 'repository_name', 'file', 'path', 'Counter', 'file', 'identifiers', 'file', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'res', 'repository_name', 'file', 'path', 'file', 'identifiers', 'file', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'tokens', 'identifier', 'file', 'identifiers', 'tokens', 'append', 'dataclasses', 'astuple', 'identifier', 'res', 'repository_name', 'file', 'path', 'tokens', 'repository_name', 'reps2files', 'keys', 'res', 'repository_name', 'file', 'reps2files', 'repository_name', 'obj', 'file', 'objects', 'len', 'obj', 'identifiers', 'gran', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'gran', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'mode', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'Counter', 'obj', 'identifiers', 'obj', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'obj', 'identifiers', 'obj', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'tokens', 'identifier', 'obj', 'identifiers', 'tokens', 'append', 'dataclasses', 'astuple', 'identifier', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'tokens', 'json', 'dump', 'res', 'fout', 'ensure_ascii', 'indent']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/saver.py', lang='Python', bag_of_tokens=['save_wabbit', 'cls', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'counter_to_wabbit', 'tokens_counter', 'Counter', 'str', 'sorted_tokens', 'sorted', 'tokens_counter', 'items', 'key', 'itemgetter', 'reverse', 'formatted_tokens', 'token', 'sorted_tokens', 'formatted_tokens', 'append', 'format', 'token', 'token', 'count', 'str', 'token', 'join', 'formatted_tokens', 'sequence_to_wabbit', 'sequence', 'Union', 'List', 'str', 'List', 'IdentifierData', 'identifiers_type', 'IdentifiersTypes', 'str', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'join', 'sequence', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'formatted_tokens', 'token', 'sequence', 'parameters', 'join', 'str', 'parameter', 'parameter', 'token', 'start_byte', 'token', 'start_line', 'token', 'start_column', 'formatted_tokens', 'append', 'format', 'token', 'token', 'identifier', 'parameters', 'parameters', 'join', 'formatted_tokens', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'output_dir', 'filename', 'fout', 'gran', 'repository_name', 'reps2files', 'keys', 'repository_tokens', 'counter_to_wabbit', 'merge_bags', 'reps2files', 'repository_name', 'len', 'repository_tokens', 'fout', 'write', 'format', 'name', 'repository_name', 'tokens', 'repository_tokens', 'gran', 'repository_name', 'reps2files', 'values', 'file', 'repository_name', 'mode', 'file_tokens', 'counter_to_wabbit', 'Counter', 'file', 'identifiers', 'file_tokens', 'sequence_to_wabbit', 'file', 'identifiers', 'file', 'identifiers_type', 'len', 'file_tokens', 'fout', 'write', 'format', 'name', 'file', 'path', 'tokens', 'file_tokens', 'repository_name', 'reps2files', 'values', 'file', 'repository_name', 'obj', 'file', 'objects', 'gran', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'gran', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'mode', 'object_tokens', 'counter_to_wabbit', 'Counter', 'obj', 'identifiers', 'object_tokens', 'sequence_to_wabbit', 'obj', 'identifiers', 'obj', 'identifiers_type', 'len', 'object_tokens', 'fout', 'write', 'format', 'name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'tokens', 'object_tokens']),\n",
       "  0.918848167539267),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/saver.py', lang='Python', bag_of_tokens=['OutputFormats', '__init__', 'self', 'output_format', 'str', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'output_format', 'self', 'save_wabbit', 'reps2files', 'mode', 'gran', 'output_dir', 'filename', 'output_format', 'self', 'save_json', 'reps2files', 'mode', 'gran', 'output_dir', 'filename', 'classmethod', 'save_wabbit', 'cls', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'counter_to_wabbit', 'tokens_counter', 'Counter', 'str', 'sorted_tokens', 'sorted', 'tokens_counter', 'items', 'key', 'itemgetter', 'reverse', 'formatted_tokens', 'token', 'sorted_tokens', 'formatted_tokens', 'append', 'format', 'token', 'token', 'count', 'str', 'token', 'join', 'formatted_tokens', 'sequence_to_wabbit', 'sequence', 'Union', 'List', 'str', 'List', 'IdentifierData', 'identifiers_type', 'IdentifiersTypes', 'str', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'join', 'sequence', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'formatted_tokens', 'token', 'sequence', 'parameters', 'join', 'str', 'parameter', 'parameter', 'token', 'start_byte', 'token', 'start_line', 'token', 'start_column', 'formatted_tokens', 'append', 'format', 'token', 'token', 'identifier', 'parameters', 'parameters', 'join', 'formatted_tokens', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'output_dir', 'filename', 'fout', 'gran', 'repository_name', 'reps2files', 'keys', 'repository_tokens', 'counter_to_wabbit', 'merge_bags', 'reps2files', 'repository_name', 'len', 'repository_tokens', 'fout', 'write', 'format', 'name', 'repository_name', 'tokens', 'repository_tokens', 'gran', 'repository_name', 'reps2files', 'values', 'file', 'repository_name', 'mode', 'file_tokens', 'counter_to_wabbit', 'Counter', 'file', 'identifiers', 'file_tokens', 'sequence_to_wabbit', 'file', 'identifiers', 'file', 'identifiers_type', 'len', 'file_tokens', 'fout', 'write', 'format', 'name', 'file', 'path', 'tokens', 'file_tokens', 'repository_name', 'reps2files', 'values', 'file', 'repository_name', 'obj', 'file', 'objects', 'gran', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'gran', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'mode', 'object_tokens', 'counter_to_wabbit', 'Counter', 'obj', 'identifiers', 'object_tokens', 'sequence_to_wabbit', 'obj', 'identifiers', 'obj', 'identifiers_type', 'len', 'object_tokens', 'fout', 'write', 'format', 'name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'tokens', 'object_tokens', 'classmethod', 'save_json', 'cls', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'res', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'output_dir', 'filename', 'fout', 'gran', 'repository_name', 'reps2files', 'keys', 'repository_tokens', 'merge_bags', 'reps2files', 'repository_name', 'len', 'repository_tokens', 'res', 'repository_name', 'repository_tokens', 'gran', 'repository_name', 'reps2files', 'keys', 'res', 'repository_name', 'file', 'reps2files', 'repository_name', 'len', 'file', 'identifiers', 'mode', 'res', 'repository_name', 'file', 'path', 'Counter', 'file', 'identifiers', 'file', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'res', 'repository_name', 'file', 'path', 'file', 'identifiers', 'file', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'tokens', 'identifier', 'file', 'identifiers', 'tokens', 'append', 'dataclasses', 'astuple', 'identifier', 'res', 'repository_name', 'file', 'path', 'tokens', 'repository_name', 'reps2files', 'keys', 'res', 'repository_name', 'file', 'reps2files', 'repository_name', 'obj', 'file', 'objects', 'len', 'obj', 'identifiers', 'gran', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'gran', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'mode', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'Counter', 'obj', 'identifiers', 'obj', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'obj', 'identifiers', 'obj', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'tokens', 'identifier', 'obj', 'identifiers', 'tokens', 'append', 'dataclasses', 'astuple', 'identifier', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'tokens', 'json', 'dump', 'res', 'fout', 'ensure_ascii', 'indent']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/saver.py', lang='Python', bag_of_tokens=['save_json', 'cls', 'reps2files', 'Dict', 'str', 'List', 'FileData', 'mode', 'str', 'gran', 'str', 'output_dir', 'str', 'filename', 'str', 'res', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'output_dir', 'filename', 'fout', 'gran', 'repository_name', 'reps2files', 'keys', 'repository_tokens', 'merge_bags', 'reps2files', 'repository_name', 'len', 'repository_tokens', 'res', 'repository_name', 'repository_tokens', 'gran', 'repository_name', 'reps2files', 'keys', 'res', 'repository_name', 'file', 'reps2files', 'repository_name', 'len', 'file', 'identifiers', 'mode', 'res', 'repository_name', 'file', 'path', 'Counter', 'file', 'identifiers', 'file', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'res', 'repository_name', 'file', 'path', 'file', 'identifiers', 'file', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'tokens', 'identifier', 'file', 'identifiers', 'tokens', 'append', 'dataclasses', 'astuple', 'identifier', 'res', 'repository_name', 'file', 'path', 'tokens', 'repository_name', 'reps2files', 'keys', 'res', 'repository_name', 'file', 'reps2files', 'repository_name', 'obj', 'file', 'objects', 'len', 'obj', 'identifiers', 'gran', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'gran', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'mode', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'Counter', 'obj', 'identifiers', 'obj', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'obj', 'identifiers', 'obj', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'tokens', 'identifier', 'obj', 'identifiers', 'tokens', 'append', 'dataclasses', 'astuple', 'identifier', 'res', 'repository_name', 'file', 'path', 'obj', 'start_line', 'obj', 'end_line', 'tokens', 'json', 'dump', 'res', 'fout', 'ensure_ascii', 'indent']),\n",
       "  0.7958115183246073),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['stem_threshold', 'self', 'value', 'isinstance', 'value', 'int', 'TypeError', 'type', 'value', 'value', 'ValueError', 'value', 'self', '_stem_threshold', 'value']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['max_token_length', 'self', 'value', 'isinstance', 'value', 'int', 'TypeError', 'type', 'value', 'value', 'ValueError', 'value', 'self', '_max_token_length', 'value']),\n",
       "  0.8666666666666667),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['stem_threshold', 'self', 'value', 'isinstance', 'value', 'int', 'TypeError', 'type', 'value', 'value', 'ValueError', 'value', 'self', '_stem_threshold', 'value']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['min_split_length', 'self', 'value', 'isinstance', 'value', 'int', 'TypeError', 'type', 'value', 'value', 'ValueError', 'value', 'self', '_min_split_length', 'value']),\n",
       "  0.8666666666666667),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['max_token_length', 'self', 'value', 'isinstance', 'value', 'int', 'TypeError', 'type', 'value', 'value', 'ValueError', 'value', 'self', '_max_token_length', 'value']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['min_split_length', 'self', 'value', 'isinstance', 'value', 'int', 'TypeError', 'type', 'value', 'value', 'ValueError', 'value', 'self', '_min_split_length', 'value']),\n",
       "  0.8666666666666667),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['meta_decorator', 'func', 'self', '_save_token_style', 'functools', 'wraps', 'func', 'decorated_func', 'name', 'name', 'isupper', 'meta', 'TokenStyle', 'TOKEN_UPPER', 'name', 'islower', 'meta', 'TokenStyle', 'TOKEN_LOWER', 'meta', 'TokenStyle', 'TOKEN_CAPITALIZED', 'res', 'func', 'name', 'res', 'meta', 'decorated_func', 'func']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/subtokenizer.py', lang='Python', bag_of_tokens=['decorated_func', 'name', 'name', 'isupper', 'meta', 'TokenStyle', 'TOKEN_UPPER', 'name', 'islower', 'meta', 'TokenStyle', 'TOKEN_LOWER', 'meta', 'TokenStyle', 'TOKEN_CAPITALIZED', 'res', 'func', 'name', 'res', 'meta']),\n",
       "  0.8275862068965517),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['TestPipeline', 'unittest', 'TestCase', 'test_languages', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'self', 'assertEqual', 'len', 'lang2files', 'self', 'assertEqual', 'lang2files', 'keys', 'test_transforming_list', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'files', 'transform_files_list', 'lang2files', 'self', 'assertEqual', 'len', 'files', 'test_tokenization', 'self', 'TemporaryDirectory', 'td', 'tokenize_list_of_repositories', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'td', 'identifiers_verbose', 'subtokenize', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'td', 'fin', 'wabbit_lines', 'sum', '_', 'fin', 'self', 'assertEqual', 'wabbit_lines']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['test_languages', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'self', 'assertEqual', 'len', 'lang2files', 'self', 'assertEqual', 'lang2files', 'keys']),\n",
       "  0.6901408450704225),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['TestPipeline', 'unittest', 'TestCase', 'test_languages', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'self', 'assertEqual', 'len', 'lang2files', 'self', 'assertEqual', 'lang2files', 'keys', 'test_transforming_list', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'files', 'transform_files_list', 'lang2files', 'self', 'assertEqual', 'len', 'files', 'test_tokenization', 'self', 'TemporaryDirectory', 'td', 'tokenize_list_of_repositories', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'td', 'identifiers_verbose', 'subtokenize', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'td', 'fin', 'wabbit_lines', 'sum', '_', 'fin', 'self', 'assertEqual', 'wabbit_lines']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['test_transforming_list', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'files', 'transform_files_list', 'lang2files', 'self', 'assertEqual', 'len', 'files']),\n",
       "  0.7183098591549296),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['TestPipeline', 'unittest', 'TestCase', 'test_languages', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'self', 'assertEqual', 'len', 'lang2files', 'self', 'assertEqual', 'lang2files', 'keys', 'test_transforming_list', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'files', 'transform_files_list', 'lang2files', 'self', 'assertEqual', 'len', 'files', 'test_tokenization', 'self', 'TemporaryDirectory', 'td', 'tokenize_list_of_repositories', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'td', 'identifiers_verbose', 'subtokenize', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'td', 'fin', 'wabbit_lines', 'sum', '_', 'fin', 'self', 'assertEqual', 'wabbit_lines']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['test_tokenization', 'self', 'TemporaryDirectory', 'td', 'tokenize_list_of_repositories', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'td', 'identifiers_verbose', 'subtokenize', 'open', 'os', 'path', 'abspath', 'os', 'path', 'join', 'td', 'fin', 'wabbit_lines', 'sum', '_', 'fin', 'self', 'assertEqual', 'wabbit_lines']),\n",
       "  0.7464788732394366),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['test_languages', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'self', 'assertEqual', 'len', 'lang2files', 'self', 'assertEqual', 'lang2files', 'keys']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_pipeline.py', lang='Python', bag_of_tokens=['test_transforming_list', 'self', 'lang2files', 'recognize_languages_dir', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'files', 'transform_files_list', 'lang2files', 'self', 'assertEqual', 'len', 'files']),\n",
       "  0.8947368421052632),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_subtokenizer.py', lang='Python', bag_of_tokens=['TestSubtokenizing', 'unittest', 'TestCase', 'test_subtokenizing_data', 'Subtokenizer', 'TokenParser', 'test_subtokenizer', 'self', 'data', 'TestSubtokenizing', 'test_subtokenizing_data', 'self', 'subTest', 'subtokens', 'list', 'TestSubtokenizing', 'Subtokenizer', 'process_token', 'data', 'self', 'assertEqual', 'subtokens', 'data']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_subtokenizer.py', lang='Python', bag_of_tokens=['test_subtokenizer', 'self', 'data', 'TestSubtokenizing', 'test_subtokenizing_data', 'self', 'subTest', 'subtokens', 'list', 'TestSubtokenizing', 'Subtokenizer', 'process_token', 'data', 'self', 'assertEqual', 'subtokens', 'data']),\n",
       "  0.8695652173913043),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_tokenizer.py', lang='Python', bag_of_tokens=['TestParser', 'unittest', 'TestCase', 'test_parser_data', 'test_parser', 'self', 'data', 'TestParser', 'test_parser_data', 'self', 'subTest', 'file', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'data', 'tokens', 'Counter', 'get_identifiers_sequence_from_file', 'file', 'data', 'identifiers_verbose', 'subtokenize', 'self', 'assertEqual', 'tokens', 'Counter', 'data']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tests/test_tokenizer.py', lang='Python', bag_of_tokens=['test_parser', 'self', 'data', 'TestParser', 'test_parser_data', 'self', 'subTest', 'file', 'os', 'path', 'abspath', 'os', 'path', 'join', 'tests_dir', 'data', 'tokens', 'Counter', 'get_identifiers_sequence_from_file', 'file', 'data', 'identifiers_verbose', 'subtokenize', 'self', 'assertEqual', 'tokens', 'Counter', 'data']),\n",
       "  0.9375),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_identifiers_sequence_from_code', 'code', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'Union', 'List', 'str', 'List', 'IdentifierData', 'code', 'bytes', 'code', 'tree', 'get_parser', 'TreeSitterParser', 'PARSERS', 'lang', 'parse', 'code', 'root', 'tree', 'root_node', 'TreeSitterParser', 'get_identifiers_sequence_from_node', 'code', 'root', 'lang', 'identifiers_verbose', 'subtokenize']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_identifiers_sequence_from_code', 'code', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'Union', 'List', 'str', 'List', 'IdentifierData', 'lang', 'SUPPORTED_LANGUAGES', 'TreeSitterParser', 'get_identifiers_sequence_from_code', 'code', 'lang', 'identifiers_verbose', 'subtokenize', 'lang', 'SUPPORTED_LANGUAGES', 'PygmentsParser', 'get_identifiers_sequence_from_code', 'code', 'lang', 'identifiers_verbose', 'subtokenize', 'ValueError']),\n",
       "  0.7058823529411765),\n",
       " (EntityData(entity_type=<EntityTypes.Class: 1>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['PygmentsParser', 'LEXERS', 'ScalaLexer', 'SwiftLexer', 'KotlinLexer', 'HaskellLexer', 'IDENTIFIERS', 'pygments', 'token', 'Name', 'pygments', 'token', 'Keyword', 'Type', 'pygments', 'token', 'Name', 'pygments', 'token', 'Name', 'pygments', 'token', 'Name', 'pygments', 'token', 'Keyword', 'Type', 'staticmethod', 'get_identifiers_sequence_from_code', 'code', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'Union', 'List', 'str', 'List', 'IdentifierData', 'tokens', 'pair', 'pygments', 'lex', 'code', 'PygmentsParser', 'LEXERS', 'lang', 'any', 'pair', 'sublist', 'sublist', 'PygmentsParser', 'IDENTIFIERS', 'lang', 'identifiers_verbose', 'token', 'pair', 'token', 'IdentifierData', 'pair', 'subtokenize', 'tokens', 'append', 'token', 'tokens', 'extend', 'subtokenize_identifier', 'token', 'tokens', 'staticmethod', 'get_data_from_file', 'file', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'FileData', 'code', 'read_file', 'file', 'identifiers', 'PygmentsParser', 'get_identifiers_sequence_from_code', 'code', 'lang', 'identifiers_verbose', 'subtokenize', 'identifiers_verbose', 'identifiers_type', 'IdentifiersTypes', 'VERBOSE', 'identifiers_type', 'IdentifiersTypes', 'STRING', 'FileData', 'path', 'file', 'lang', 'lang', 'objects', 'identifiers', 'identifiers', 'identifiers_type', 'identifiers_type']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_identifiers_sequence_from_code', 'code', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'Union', 'List', 'str', 'List', 'IdentifierData', 'tokens', 'pair', 'pygments', 'lex', 'code', 'PygmentsParser', 'LEXERS', 'lang', 'any', 'pair', 'sublist', 'sublist', 'PygmentsParser', 'IDENTIFIERS', 'lang', 'identifiers_verbose', 'token', 'pair', 'token', 'IdentifierData', 'pair', 'subtokenize', 'tokens', 'append', 'token', 'tokens', 'extend', 'subtokenize_identifier', 'token', 'tokens']),\n",
       "  0.6909090909090909),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_identifiers_sequence_from_code', 'code', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'Union', 'List', 'str', 'List', 'IdentifierData', 'lang', 'SUPPORTED_LANGUAGES', 'TreeSitterParser', 'get_identifiers_sequence_from_code', 'code', 'lang', 'identifiers_verbose', 'subtokenize', 'lang', 'SUPPORTED_LANGUAGES', 'PygmentsParser', 'get_identifiers_sequence_from_code', 'code', 'lang', 'identifiers_verbose', 'subtokenize', 'ValueError']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_identifiers_sequence_from_file', 'file', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'Union', 'List', 'str', 'List', 'IdentifierData', 'code', 'read_file', 'file', 'get_identifiers_sequence_from_code', 'code', 'lang', 'identifiers_verbose', 'subtokenize']),\n",
       "  0.8387096774193549),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_data_from_file', 'file', 'str', 'lang', 'str', 'gather_objects', 'bool', 'gather_identifiers', 'bool', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'FileData', 'logging', 'debug', 'file', 'lang', 'SUPPORTED_LANGUAGES', 'TreeSitterParser', 'get_data_from_file', 'file', 'lang', 'gather_objects', 'gather_identifiers', 'identifiers_verbose', 'subtokenize', 'lang', 'SUPPORTED_LANGUAGES', 'PygmentsParser', 'get_data_from_file', 'file', 'lang', 'identifiers_verbose', 'subtokenize', 'ValueError', 'UnicodeDecodeError', 'logging', 'warning', 'file', 'FileData', 'path', 'file', 'lang', 'lang', 'objects', 'identifiers', 'identifiers_type', 'IdentifiersTypes', 'STRING']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_functions_from_file', 'file', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'List', 'ObjectData', 'lang', 'SUPPORTED_LANGUAGES', 'ValueError', 'lang', 'file_data', 'TreeSitterParser', 'get_data_from_file', 'file', 'lang', 'gather_objects', 'gather_identifiers', 'identifiers_verbose', 'identifiers_verbose', 'subtokenize', 'subtokenize', 'obj', 'file_data', 'objects', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'obj']),\n",
       "  0.74),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_data_from_file', 'file', 'str', 'lang', 'str', 'gather_objects', 'bool', 'gather_identifiers', 'bool', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'FileData', 'logging', 'debug', 'file', 'lang', 'SUPPORTED_LANGUAGES', 'TreeSitterParser', 'get_data_from_file', 'file', 'lang', 'gather_objects', 'gather_identifiers', 'identifiers_verbose', 'subtokenize', 'lang', 'SUPPORTED_LANGUAGES', 'PygmentsParser', 'get_data_from_file', 'file', 'lang', 'identifiers_verbose', 'subtokenize', 'ValueError', 'UnicodeDecodeError', 'logging', 'warning', 'file', 'FileData', 'path', 'file', 'lang', 'lang', 'objects', 'identifiers', 'identifiers_type', 'IdentifiersTypes', 'STRING']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_classes_from_file', 'file', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'List', 'ObjectData', 'lang', 'SUPPORTED_LANGUAGES', 'ValueError', 'lang', 'file_data', 'TreeSitterParser', 'get_data_from_file', 'file', 'lang', 'gather_objects', 'gather_identifiers', 'identifiers_verbose', 'identifiers_verbose', 'subtokenize', 'subtokenize', 'obj', 'file_data', 'objects', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'obj']),\n",
       "  0.74),\n",
       " (EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_functions_from_file', 'file', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'List', 'ObjectData', 'lang', 'SUPPORTED_LANGUAGES', 'ValueError', 'lang', 'file_data', 'TreeSitterParser', 'get_data_from_file', 'file', 'lang', 'gather_objects', 'gather_identifiers', 'identifiers_verbose', 'identifiers_verbose', 'subtokenize', 'subtokenize', 'obj', 'file_data', 'objects', 'obj', 'object_type', 'ObjectTypes', 'FUNCTION', 'obj']),\n",
       "  EntityData(entity_type=<EntityTypes.Function: 0>, path='/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection/duplication/tokenizer/buckwheat/tokenizer.py', lang='Python', bag_of_tokens=['get_classes_from_file', 'file', 'str', 'lang', 'str', 'identifiers_verbose', 'bool', 'subtokenize', 'bool', 'List', 'ObjectData', 'lang', 'SUPPORTED_LANGUAGES', 'ValueError', 'lang', 'file_data', 'TreeSitterParser', 'get_data_from_file', 'file', 'lang', 'gather_objects', 'gather_identifiers', 'identifiers_verbose', 'identifiers_verbose', 'subtokenize', 'subtokenize', 'obj', 'file_data', 'objects', 'obj', 'object_type', 'ObjectTypes', 'CLASS', 'obj']),\n",
       "  0.9411764705882353)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = get_data_from_file(test_dir/\"test_file.java\", \"Java\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[obj for obj in file_data.objects if obj.object_type == ObjectTypes.FUNCTION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
