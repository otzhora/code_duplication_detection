{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd066fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5 (default, Jan 27 2021, 15:41:15) \n",
      "[GCC 9.3.0] on linux\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "sys.path.extend(['/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection', '/mnt/c/Users/Yuriy Rogachev/PycharmProjects/code duplication detection'])\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "cwd = os.getcwd()\n",
    "test_dir = Path(cwd).parent/\"duplication\"/\"test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a514d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa7f6e1",
   "metadata": {},
   "source": [
    "# Labrary testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ee1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duplication.detectors import NaiveDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d909a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_detector = NaiveDetector()\n",
    "clones = naive_detector.detect(test_dir.parent, 0.6, \"functions\").clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cab7f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(clones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdb48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cloen(clone):\n",
    "    print(clone[0].object_data.content)\n",
    "    print()\n",
    "    print(clone[1].object_data.content)\n",
    "    print()\n",
    "    print(clone[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d305ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def some_func1(a):\n",
      "    s = 0\n",
      "    for x in a:\n",
      "        s += x\n",
      "    return s\n",
      "\n",
      "def some_func1(a):\n",
      "    s = 0\n",
      "    for x in a:\n",
      "        s += x\n",
      "        s += x ** 2\n",
      "    return s\n",
      "\n",
      "0.8\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_enry_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get the directory with Enry.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"build\"))\n",
      "\n",
      "def get_enry() -> str:\n",
      "    \"\"\"\n",
      "    Get the path to the Enry binary.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(get_enry_dir(), \"enry\"))\n",
      "\n",
      "0.6666666666666666\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_enry_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get the directory with Enry.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"build\"))\n",
      "\n",
      "def get_tree_sitter_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get tree-sitter directory.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"build\"))\n",
      "\n",
      "0.9166666666666666\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_enry_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get the directory with Enry.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"build\"))\n",
      "\n",
      "def get_tree_sitter_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get tree-sitter directory.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.dirname(__file__))\n",
      "\n",
      "0.6666666666666666\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_enry() -> str:\n",
      "    \"\"\"\n",
      "    Get the path to the Enry binary.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(get_enry_dir(), \"enry\"))\n",
      "\n",
      "def get_tree_sitter_so() -> str:\n",
      "    \"\"\"\n",
      "    Get build tree-sitter `.so` location.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(get_tree_sitter_dir(), \"langs.so\"))\n",
      "\n",
      "0.7777777777777778\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_enry() -> str:\n",
      "    \"\"\"\n",
      "    Get the path to the Enry binary.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(get_enry_dir(), \"enry\"))\n",
      "\n",
      "def get_tree_sitter_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get tree-sitter directory.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.dirname(__file__))\n",
      "\n",
      "0.6666666666666666\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def recognize_languages_dir(directory: str) -> Dict[str, List[str]]:\n",
      "    \"\"\"\n",
      "    Recognize the languages in the directory using Enry and return a dictionary.\n",
      "    {language1: [files], language2: [files], ...}.\n",
      "    :param directory: the path to the directory.\n",
      "    :return: dictionary {language1: [files], language2: [files], ...}\n",
      "    \"\"\"\n",
      "    enry = get_enry()\n",
      "    args = [enry, \"-json\", directory]\n",
      "    res = subprocess.check_output(args)\n",
      "    return json.loads(res)\n",
      "\n",
      "def recognize_language_file(file_path: str) -> Dict[str, str]:\n",
      "    \"\"\"\n",
      "    Recognize the language of a file.\n",
      "    :param file_path: directory location to classify.\n",
      "    :return: dictionary `{\"filename\":name,\"language\":lang,\"lines\":n_lines,\"mime\":mime,\"total_lines\":n_total_lines,\n",
      "                         \"type\":type,\"vendored\":bool}`\n",
      "    \"\"\"\n",
      "    if not os.path.isfile(file_path):\n",
      "        raise ValueError(\"Expected path to file path but got '%s'\" % file_path)\n",
      "    enry = get_enry()\n",
      "    args = [enry, \"-json\", file_path]\n",
      "    res = subprocess.check_output(args)\n",
      "    return json.loads(res)\n",
      "\n",
      "0.625\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_tree_sitter_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get tree-sitter directory.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"build\"))\n",
      "\n",
      "def get_tree_sitter_so() -> str:\n",
      "    \"\"\"\n",
      "    Get build tree-sitter `.so` location.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(get_tree_sitter_dir(), \"langs.so\"))\n",
      "\n",
      "0.6666666666666666\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_tree_sitter_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get tree-sitter directory.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"build\"))\n",
      "\n",
      "def get_tree_sitter_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get tree-sitter directory.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.dirname(__file__))\n",
      "\n",
      "0.75\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_tree_sitter_so() -> str:\n",
      "    \"\"\"\n",
      "    Get build tree-sitter `.so` location.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.join(get_tree_sitter_dir(), \"langs.so\"))\n",
      "\n",
      "def get_tree_sitter_dir() -> str:\n",
      "    \"\"\"\n",
      "    Get tree-sitter directory.\n",
      "    :return: absolute path.\n",
      "    \"\"\"\n",
      "    return os.path.abspath(os.path.dirname(__file__))\n",
      "\n",
      "0.7777777777777778\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def stem_threshold(self, value):\n",
      "        if not isinstance(value, int):\n",
      "            raise TypeError(\"stem_threshold must be an integer - got %s\" % type(value))\n",
      "        if value < 1:\n",
      "            raise ValueError(\"stem_threshold must be greater than 0 - got %d\" % value)\n",
      "        self._stem_threshold = value\n",
      "\n",
      "def max_token_length(self, value):\n",
      "        if not isinstance(value, int):\n",
      "            raise TypeError(\"max_token_length must be an integer - got %s\" % type(value))\n",
      "        if value < 1:\n",
      "            raise ValueError(\"max_token_length must be greater than 0 - got %d\" % value)\n",
      "        self._max_token_length = value\n",
      "\n",
      "0.8666666666666667\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def stem_threshold(self, value):\n",
      "        if not isinstance(value, int):\n",
      "            raise TypeError(\"stem_threshold must be an integer - got %s\" % type(value))\n",
      "        if value < 1:\n",
      "            raise ValueError(\"stem_threshold must be greater than 0 - got %d\" % value)\n",
      "        self._stem_threshold = value\n",
      "\n",
      "def min_split_length(self, value):\n",
      "        if not isinstance(value, int):\n",
      "            raise TypeError(\"min_split_length must be an integer - got %s\" % type(value))\n",
      "        if value < 1:\n",
      "            raise ValueError(\"min_split_length must be greater than 0 - got %d\" % value)\n",
      "        self._min_split_length = value\n",
      "\n",
      "0.8666666666666667\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def max_token_length(self, value):\n",
      "        if not isinstance(value, int):\n",
      "            raise TypeError(\"max_token_length must be an integer - got %s\" % type(value))\n",
      "        if value < 1:\n",
      "            raise ValueError(\"max_token_length must be greater than 0 - got %d\" % value)\n",
      "        self._max_token_length = value\n",
      "\n",
      "def min_split_length(self, value):\n",
      "        if not isinstance(value, int):\n",
      "            raise TypeError(\"min_split_length must be an integer - got %s\" % type(value))\n",
      "        if value < 1:\n",
      "            raise ValueError(\"min_split_length must be greater than 0 - got %d\" % value)\n",
      "        self._min_split_length = value\n",
      "\n",
      "0.8666666666666667\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def meta_decorator(func):\n",
      "            if self._save_token_style:\n",
      "                @functools.wraps(func)\n",
      "                def decorated_func(name):\n",
      "                    if name.isupper():\n",
      "                        meta = TokenStyle.TOKEN_UPPER\n",
      "                    elif name.islower():\n",
      "                        meta = TokenStyle.TOKEN_LOWER\n",
      "                    else:\n",
      "                        meta = TokenStyle.TOKEN_CAPITALIZED\n",
      "                    for res in func(name):\n",
      "                        yield res, meta\n",
      "\n",
      "                return decorated_func\n",
      "            else:\n",
      "                return func\n",
      "\n",
      "def decorated_func(name):\n",
      "                    if name.isupper():\n",
      "                        meta = TokenStyle.TOKEN_UPPER\n",
      "                    elif name.islower():\n",
      "                        meta = TokenStyle.TOKEN_LOWER\n",
      "                    else:\n",
      "                        meta = TokenStyle.TOKEN_CAPITALIZED\n",
      "                    for res in func(name):\n",
      "                        yield res, meta\n",
      "\n",
      "0.6896551724137931\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def test_languages(self):\n",
      "        lang2files = recognize_languages_dir(os.path.abspath(os.path.join(tests_dir, \"test_files\")))\n",
      "        self.assertEqual(len(lang2files), 16)\n",
      "        self.assertEqual(lang2files.keys(),\n",
      "                         {\"C\", \"C#\", \"C++\", \"Go\", \"Haskell\", \"Java\", \"JavaScript\", \"Kotlin\", \"PHP\",\n",
      "                          \"Python\", \"Ruby\", \"Rust\", \"Scala\", \"Shell\", \"Swift\", \"TypeScript\"})\n",
      "\n",
      "def test_transforming_list(self):\n",
      "        lang2files = recognize_languages_dir(os.path.abspath(os.path.join(tests_dir, \"test_files\")))\n",
      "        files = transform_files_list(lang2files, \"projects\", None)\n",
      "        self.assertEqual(len(files), 16)\n",
      "\n",
      "0.7368421052631579\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_identifiers_sequence_from_code(code: str, lang: str, identifiers_verbose: bool = False,\n",
      "                                           subtokenize: bool = False) -> \\\n",
      "            Union[List[str], List[IdentifierData]]:\n",
      "        \"\"\"\n",
      "        Given the code and its language, gather identifiers in it.\n",
      "        :param code: source code as a string.\n",
      "        :param lang: language of the code.\n",
      "        :param identifiers_verbose: if True, will save not only identifiers themselves,\n",
      "                                    but also their parameters as IdentifierData.\n",
      "        :param subtokenize: if True, will split the tokens into subtokens.\n",
      "        :return: list of identifiers as either strings or IdentifierData objects.\n",
      "        \"\"\"\n",
      "        code = bytes(code, \"utf-8\")\n",
      "        tree = get_parser(TreeSitterParser.PARSERS[lang]).parse(code)\n",
      "        root = tree.root_node\n",
      "        return TreeSitterParser.get_identifiers_sequence_from_node(code, root, lang,\n",
      "                                                                   identifiers_verbose,\n",
      "                                                                   subtokenize)\n",
      "\n",
      "def get_identifiers_sequence_from_code(code: str, lang: str, identifiers_verbose: bool = False,\n",
      "                                       subtokenize: bool = False) -> \\\n",
      "        Union[List[str], List[IdentifierData]]:\n",
      "    \"\"\"\n",
      "    Given the code and its language, gather its identifiers.\n",
      "    :param code: the code to parse.\n",
      "    :param lang: the language of the code fragment.\n",
      "    :param identifiers_verbose: if True, will save not only identifiers themselves,\n",
      "                                but also their parameters as IdentifierData.\n",
      "    :param subtokenize: if True, will split the tokens into subtokens.\n",
      "    :return: list of identifiers as either strings or IdentifierData objects.\n",
      "    \"\"\"\n",
      "    if lang in SUPPORTED_LANGUAGES[\"tree-sitter\"]:\n",
      "        return TreeSitterParser.get_identifiers_sequence_from_code(code, lang, identifiers_verbose,\n",
      "                                                                   subtokenize)\n",
      "    elif lang in SUPPORTED_LANGUAGES[\"pygments\"]:\n",
      "        return PygmentsParser.get_identifiers_sequence_from_code(code, lang, identifiers_verbose,\n",
      "                                                                 subtokenize)\n",
      "    else:\n",
      "        raise ValueError(\"Unsupported language!\")\n",
      "\n",
      "0.6176470588235294\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def get_functions_from_file(file: str, lang: str, identifiers_verbose: bool = False,\n",
      "                            subtokenize: bool = False) -> List[ObjectData]:\n",
      "    \"\"\"\n",
      "    Yield ObjectData objects for functions in a given file.\n",
      "    :param file: the path to file.\n",
      "    :param lang: the language of the file.\n",
      "    :param identifiers_verbose: if True, will save not only identifiers themselves,\n",
      "                                but also their parameters as IdentifierData.\n",
      "    :param subtokenize: if True, will split the tokens into subtokens.\n",
      "    :return: an iterator of ObjectData objects for functions.\n",
      "    \"\"\"\n",
      "    if lang not in SUPPORTED_LANGUAGES[\"functions\"]:\n",
      "        raise ValueError(f\"{lang} doesn't support gathering functions!\")\n",
      "    file_data = TreeSitterParser.get_data_from_file(file, lang, gather_objects=True,\n",
      "                                                    gather_identifiers=False,\n",
      "                                                    identifiers_verbose=identifiers_verbose,\n",
      "                                                    subtokenize=subtokenize)\n",
      "    for obj in file_data.objects:\n",
      "        if obj.object_type == ObjectTypes.FUNCTION:\n",
      "            yield obj\n",
      "\n",
      "def get_classes_from_file(file: str, lang: str, identifiers_verbose: bool = False,\n",
      "                          subtokenize: bool = False) -> List[ObjectData]:\n",
      "    \"\"\"\n",
      "    Yield ObjectData objects for classes in a given file.\n",
      "    :param file: the path to file.\n",
      "    :param lang: the language of the file.\n",
      "    :param identifiers_verbose: if True, will save not only identifiers themselves,\n",
      "                                but also their parameters as IdentifierData.\n",
      "    :param subtokenize: if True, will split the tokens into subtokens.\n",
      "    :return: an iterator of ObjectData objects for classes.\n",
      "    \"\"\"\n",
      "    if lang not in SUPPORTED_LANGUAGES[\"classes\"]:\n",
      "        raise ValueError(f\"{lang} doesn't support gathering functions!\")\n",
      "    file_data = TreeSitterParser.get_data_from_file(file, lang, gather_objects=True,\n",
      "                                                    gather_identifiers=False,\n",
      "                                                    identifiers_verbose=identifiers_verbose,\n",
      "                                                    subtokenize=subtokenize)\n",
      "    for obj in file_data.objects:\n",
      "        if obj.object_type == ObjectTypes.CLASS:\n",
      "            yield obj\n",
      "\n",
      "0.9411764705882353\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for clone in clones:\n",
    "    display_cloen(clone)\n",
    "    print(\"-\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cd37f",
   "metadata": {},
   "source": [
    "Good example of clones that it finds \n",
    "```python\n",
    "def get_functions_from_file(file: str, lang: str, identifiers_verbose: bool = False,\n",
    "                            subtokenize: bool = False) -> List[ObjectData]:\n",
    "    \"\"\"\n",
    "    Yield ObjectData objects for functions in a given file.\n",
    "    :param file: the path to file.\n",
    "    :param lang: the language of the file.\n",
    "    :param identifiers_verbose: if True, will save not only identifiers themselves,\n",
    "                                but also their parameters as IdentifierData.\n",
    "    :param subtokenize: if True, will split the tokens into subtokens.\n",
    "    :return: an iterator of ObjectData objects for functions.\n",
    "    \"\"\"\n",
    "    if lang not in SUPPORTED_LANGUAGES[\"functions\"]:\n",
    "        raise ValueError(f\"{lang} doesn't support gathering functions!\")\n",
    "    file_data = TreeSitterParser.get_data_from_file(file, lang, gather_objects=True,\n",
    "                                                    gather_identifiers=False,\n",
    "                                                    identifiers_verbose=identifiers_verbose,\n",
    "                                                    subtokenize=subtokenize)\n",
    "    for obj in file_data.objects:\n",
    "        if obj.object_type == ObjectTypes.FUNCTION:\n",
    "            yield obj\n",
    "\n",
    "def get_classes_from_file(file: str, lang: str, identifiers_verbose: bool = False,\n",
    "                          subtokenize: bool = False) -> List[ObjectData]:\n",
    "    \"\"\"\n",
    "    Yield ObjectData objects for classes in a given file.\n",
    "    :param file: the path to file.\n",
    "    :param lang: the language of the file.\n",
    "    :param identifiers_verbose: if True, will save not only identifiers themselves,\n",
    "                                but also their parameters as IdentifierData.\n",
    "    :param subtokenize: if True, will split the tokens into subtokens.\n",
    "    :return: an iterator of ObjectData objects for classes.\n",
    "    \"\"\"\n",
    "    if lang not in SUPPORTED_LANGUAGES[\"classes\"]:\n",
    "        raise ValueError(f\"{lang} doesn't support gathering functions!\")\n",
    "    file_data = TreeSitterParser.get_data_from_file(file, lang, gather_objects=True,\n",
    "                                                    gather_identifiers=False,\n",
    "                                                    identifiers_verbose=identifiers_verbose,\n",
    "                                                    subtokenize=subtokenize)\n",
    "    for obj in file_data.objects:\n",
    "        if obj.object_type == ObjectTypes.CLASS:\n",
    "            yield obj\n",
    "```\n",
    "\n",
    "how it should look ideally\n",
    "\n",
    "```python\n",
    "def get_objects_from_file(file: str, lang: str, object_type: ObjectsTypes, identifiers_verbose: bool = False,\n",
    "                            subtokenize: bool = False) -> List[ObjectData]:\n",
    "    \"\"\"\n",
    "    Yield ObjectData objects for functions in a given file.\n",
    "    :param file: the path to file.\n",
    "    :param lang: the language of the file.\n",
    "    :param identifiers_verbose: if True, will save not only identifiers themselves,\n",
    "                                but also their parameters as IdentifierData.\n",
    "    :param subtokenize: if True, will split the tokens into subtokens.\n",
    "    :return: an iterator of ObjectData objects for functions.\n",
    "    \"\"\"\n",
    "    if lang not in SUPPORTED_LANGUAGES[object_type.ToString()]:\n",
    "        raise ValueError(f\"{lang} doesn't support gathering {object_type.ToString()}!\")\n",
    "    file_data = TreeSitterParser.get_data_from_file(file, lang, gather_objects=True,\n",
    "                                                    gather_identifiers=False,\n",
    "                                                    identifiers_verbose=identifiers_verbose,\n",
    "                                                    subtokenize=subtokenize)\n",
    "    for obj in file_data.objects:\n",
    "        if obj.object_type == object_type:\n",
    "            yield obj\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa74b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
